\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textcomp} % Euro symbols etc
\usepackage{graphicx} % support graphics
\usepackage{hyperref} % links in the document
\usepackage{float} % position of figures
\usepackage{paralist} % inline lists
\usepackage{verbatim} % multi-line comments
\usepackage{listings} % Syntax colored code
\usepackage{booktabs} % Professional tables
\usepackage{tabularx} % Simple column stretching
\usepackage{multirow} % Row spanning
\usepackage{wrapfig} % Wrap text around figures
\usepackage{amsmath} % Advanced maths
\usepackage{array}
\usepackage[normalsize, bf]{caption}
\usepackage{color}
\usepackage{textcomp}
\usepackage{fixltx2e}
%\usepackage{fullpage} % smaller margins

\setcounter{tocdepth}{2} % Depth of table of contents

% Configure links in pdfs
\hypersetup{
    bookmarksopen=false, % Hide bookmarks menu
    colorlinks=true, % Don't wrap links in colored boxes
%    pdfborder={0 0 0} % Remove ugly boxes
}


%============
% Top matter
%============
\title{TDT4215 Web-intelligence\\Project task 4: evaluation}
\author{Group 1:\\Even Wiik Thomassen, Terje Snarby, Weilin Wang}
\date{\today}

\begin{document}

%\begin{abstract}
%Your abstract goes here
%\end{abstract}

\maketitle
\tableofcontents
%\newpage


%=====================
\section{Introduction}
%=====================
This paper presents task 4 of the project in TDT4215 Web-intelligence at
Norwegian University of Science and Technology (NTNU). It will
describe the method we used to solve task 3 (\autoref{sec:method}),
the results from task 3 (\autoref{sec:result}),
and evaluation of the results (\autoref{sec:evaluation}).

Task 3 is to calculate similarities between clinical notes and therapy
chapters in Norsk legemiddelhåndbok, and rank them using a vector model.
Task 4 is to evaluate our results of task 3, and propose methods for
automatical evaluation of the results.


%===============
\section{Method}
%===============
\label{sec:method}
This section describe methods used to solve task 3.

\subsection{Preprocessing and parsing}
%-------------------------------------
Patient cases were provided as a Word file, which included eight cases. We
created a text file for each case, and made sure they were in utf-8 charset.

Therapy chapters from Norsk legemiddelhåndbok were provided as HTML files,
invalid html5 files in iso-8859-1 charset. We first preprocessed these files
by removing some of the HTML tags to make them easier to parse, and we
converted them to utf-8 charset.

We created a custom parser for parsing therapy chapters, based on Python's
HTMLParser. We parse one HTML file at a time, creating Therapy objects for
each chapter or sub*-chapter. The text found in these chapters are stored
on the objects. We stored links as a list on each object, while we preserved
their text in the object text. Sections which list relevant drugs we removed
from the text but stored the links in case they might be used later.

We stored the output of parsing in JSON format, so they could be saved to disk
between runs and loaded quickly.

\subsection{Stopwords}
%---------------------
To reduce the number of terms in documents, and to remove words which provide
little or no relevant information value, we removed so called stopwords.
We used a list of Norwegian stopwords in both bokmål and nynorsk which we
found online, and we added a few words ourself. A list of these stopwords can
be found in \autoref{tab:stopwords}, and patient cases with stopwords removed
can be found in \autoref{appendix}.

\subsection{Calculate similarities}
%----------------------------------
We decided to use a vector model for calculating the similarities between
patient cases and therapy chapters. For each document, both patient cases
and therapy chapters, we created a vector with all the terms and their
TF-IDF value. These document-vectors gives us a pseudo term-document matrix,
without having to create an actual matrix. As there were over 30,000 terms
and almost 1000 documents, a full term-document matrix would have 30M fields.
We use the log normalization version of TF weight, and inverse frequency for
IDF weight, as can be seen in (\ref{eq:tfidf}). \( f_{i,j} \) is the frequency
of term \( i \) in document \( j \), \( N \) is the total number of documents,
while \( n_{i} \) is the document frequency of term \( i \).
\begin{equation} \label{eq:tfidf}
	w_{i,j} =
	\begin{cases}
		(1 + \log f_{i,j}) \times \log(\frac{N}{n_{i}}) & \text{if } f_{i,j} > 0 \\
		0												& \text{otherwise}
	\end{cases}
\end{equation}

For each clinical note (which we have defined to be a patient case), we
calculate the similarities with all therapy chapter vectors. The similarity is
calculated as the \emph{cosine of the angle} between the two vectors, as seen
in (\ref{eq:sim}). \( d_{j} \) is the term vector of therapy chapter \( j \),
\( q \) is a patient case vector and \( t \) is the total number of terms.
\begin{align} \label{eq:sim}
	sim(d_{j}, q) &= \frac{\vec{d_{j}} \bullet \vec{q}}{|\vec{d}| \times |\vec{q}|} \nonumber \\
				  &= \frac{\sum_{i=1}^{t} w_{i,j} \times w_{i,q}}{\sqrt{\sum_{i=1}^{t} w_{i,j}^2} \times \sqrt{\sum_{i=1}^{t} w_{i,q}^2}}
\end{align}
We sort the list of results based on their similarity score (highest first),
and return the first 10 results.


%===============
\section{Result}
%===============
\label{sec:result}
The results of task 3 can be seen in \autoref{tab:task3a} and
\autoref{tab:task3b}. Case in the tables are patient case/clinical note
number and rank is the rank of the relevant therapy chapter.
\include{task3}


%===================
\section{Evaluation}
%===================
\label{sec:evaluation}
Evaluating results for a given query give great insight in what level of quality and correctness you can expect from an information retrieval system. Doing this evaluation requires human relevance judgement.

The importance of the evaluation can be seen in the context of the mandatory assignment; if a doctor were to base his/her treatment of a patient, based on the results of our information retrieval system, it is crucial to know how reliable the results are. A consequence of incorrect results could lead to wrong treatment or even death.

\subsection{Manual evaluation}
%-----------------------------
TODO

\subsection{Automatic evaluation}
%--------------------------------
Every good information retrieval system require evaluation of the results. This is a complex task, which is difficult and time consuming for humans to do. It would be beneficial to be able to automatically evaluate the results.

In the following subsections, we propose methods and techniques that could be used to make automatic evaluation of our search results.

\begin{itemize}
\item experts
\item EVEN
\end{itemize}


%=====================
% Section: Appendices
%=====================
\appendix
\section{Appendix}
\label{appendix}
This section list patient cases used as input in this project, with Norwegian
stopwords removed. The stopwords are listed in \autoref{tab:stopwords}.
\include{stopwords}
\include{cases}

\end{document}

