%---------------
\chapter{Method}
%---------------
\label{cha:method}
This section describes methods used to solve the tasks of the assignment, including
preprocessing and parsing of input files.


\section{Preprocessing and parsing}
%----------------------------------
We preprocess and parse input data files before we save them to disk as JSON
files. This is to allow us to run a task that might be dependent on other tasks, without having to re-run previous tasks. We also store task
results as JSON files.

\subsection{ICD-10}
ICD-10 were provided in an ownl2 file. We simply parsed it as a normal XML
file, with the ElementTree module in the Python standard library.
Nodes named umls\_tui, umls\_conceptId, and umls\_atomId we throw away.
Nodes named underterm, synonym, inclusion, and exclusion we store as list of
strings. The rest of the nodes are handled as strings. All these values we
store as attributes on a single ICD object. ICD-10 codes that lack both label
and code\_compacted we also throw away.

\subsection{ATC}
Parsing of ATC is very staight forward. Each line represent a code, title
pair where codes are not unique.

\subsection{Therapy chapters}
Therapy chapters from ``Norsk legemiddelhåndbok'' were provided as\\
HTML files, invalid html5 files in iso-8859-1 charset. We first preprocessed
these files by removing some of the HTML tags to make them easier to parse,
and we converted them to utf-8 charset.

We created a custom parser for parsing therapy chapters, based on\\
Python's HTMLParser. We parse one HTML file at a time, creating Therapy
objects for each chapter or sub*-chapter. The text found in these chapters are
stored on the objects. We stored links as a list on each object, while we
preserved their text in the object text. Sections which list relevant drugs
were removed from the text but stored as they might be useful later.

We manually removed subchapter T17.2 and T19.7 as they had no title nor
contained any text.

\subsection{Patient cases}
Patient cases were provided as a Word file, which included eight cases. We
created a text file for each case, and made sure they were in utf-8 charset.


\section{Stopwords}
%------------------
To reduce the number of terms in documents, and to remove words which provide
little or no relevant information value, we removed stopwords.
We used a list of Norwegian stopwords in both Bokmål and Norwegian Nynorsk
which we found
online\footnote{Stopword source: \url{http://www.wisweb.no/999/147/33899-170.html}},
and we added a few words ourself. A complete list of these stopwords can
be found in \autoref{tab:stopwords} in \autoref{appendix}.


\section{Task 1: Autocoding ICD-10}
%----------------------------------
The first task was to find the most relevant ICD-10 codes for each sentence in the cases. The ICD-10 codes have multiple fields that can be used to make this classification. We decided to solve task 1 in two different ways.
\begin{enumerate}
\item For each ICD-10 code, use only the label to find relevant codes for each sentence in the cases. 
\item For each ICD-10 code, use all text  to find relevant codes for each sentence in the cases.
\end{enumerate}
We have decided to define all text as: label, terms, synonyms and inclusions.

\paragraph{Scoring algorithm}
To score and sort the results, we use the scoring module given by Whoosh\footnote{Scoring module: \url{http://packages.python.org/Whoosh/api/scoring.html\#whoosh.scoring}}. The module use the scoring algorithm BM25F to calculate the rankings of each code, and it returns a ranked list over relevant codes for each sentence.

BM25F\footnote{BM25F algorithm: \url{http://en.wikipedia.org/wiki/Okapi_BM25}} is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in a document. It calculates a score for each document based on the query. 

\begin{equation} \label{eq:bm25f}
	\text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
\end{equation}

\begin{equation} \label{eq:idf}
	\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
\end{equation}\\

In our case the set of documents is all the ICD-10 codes. Each code gets a score, given by \autoref{eq:bm25f}, where D is ICD-10 code and Q is a sentence from the patient case. 


\section{Task 2: Autocoding ATC}
%-------------------------------


\section{Task 3: Ranking using vector models}
%--------------------------------------------
\label{sec:task3}
We decided to use a vector model for calculating the similarities between
patient cases and therapy chapters. For each document, both patient cases
and therapy chapters, we created a vector with all the terms and their
TF-IDF value. These document-vectors gives us a pseudo term-document matrix,
without having to create an actual matrix. As there were over 30,000 terms
and almost 1000 documents, a full term-document matrix would have 30M cells.
Our pseudo term-document only have 124,065 term-weight pairs.

We have tested four different varieties of TF-IDF. For TF we tested log
normalization (\( 1 + \log f_{i,j} \)) and raw frequency (\( f_{i,j} \)).
For IDF we tested inverse frequency (\( \log(\frac{N}{n_{i}}) \)) and
probabilistic inverse frequency (\( \log(\frac{N - n_{i}}{n_{i}}) \)).
\begin{description}
	\item[A] log normalization and inverse frequency as seen in \autoref{eq:tfidf}
	\item[B] log normalization and probabilistic inverse frequency
	\item[C] raw frequency and inverse frequency 
	\item[D] raw frequency and probabilistic inverse frequency
\end{description}
Function \( f_{i,j} \) is the frequency of term \( i \) in document
\( j \), \( N \) is the total number of documents, while \( n_{i} \) is the
document frequency of term \( i \).
\begin{equation} \label{eq:tfidf}
	w_{i,j} =
	\begin{cases}
		(1 + \log f_{i,j}) \times \log(\frac{N}{n_{i}}) & \text{if } f_{i,j} > 0 \\
		0												& \text{otherwise}
	\end{cases}
\end{equation}

For each clinical note (which we have defined to be a patient case), we
calculated the similarities with all therapy chapter vectors. The similarity is
calculated as the \emph{cosine of the angle} between the two vectors, as seen
in \autoref{eq:sim}, where \( d_{j} \) is the term vector of therapy chapter \( j \),
\( q \) is a patient case vector and \( t \) is the total number of terms.
\begin{align} \label{eq:sim}
	sim(d_{j}, q) &= \frac{\vec{d_{j}} \bullet \vec{q}}{|\vec{d}| \times |\vec{q}|} \nonumber \\
				  &= \frac{\sum_{i=1}^{t} w_{i,j} \times w_{i,q}}{\sqrt{\sum_{i=1}^{t} w_{i,j}^2} \times \sqrt{\sum_{i=1}^{t} w_{i,q}^2}}
\end{align}
We sorted results based on their similarity score (highest first), and
returned the first 10 results.


\section{Task 4: Evaluation}
%---------------------------
\label{sec:task4}
The problem with automatic evaluation is deciding if a retrieved document
is relevant or not. As the queries in this task is patient cases, which is
known beforehand, we can create a criteria for whether a document is relevant
or not. We have simply taken all the words in the patient cases and created a
list of those with medical importance, and if both the query and a retrieved
document contains such a word we deem it relevant. These medical terms are
listed in \autoref{tab:medicalterms} in \hyperref[appendix]{Appendix}.

We have calculated average precision at ten documents seen (P@10) and
R-precision for the four different TF-IDF definitions A to D as described in
\autoref{sec:task3}.

We have also calculated the Kendall tau coefficients for the four ranking
methods. Kendall tau coefficient is a rank correlation metric for automatic
comparing of two ranking methods to determine how differently one varies from
another. It does not consider relevance of retrieved documents, only the
relative ordering of two rankings.


\section{Task 5: Exchange evaluations}
%-------------------------------------


\section{Task 6: Improving the ranking}
%--------------------------------------
For part A we process task 1 and 2 results by giving each patient case a list
of relevant ICD-10 and ATC codes, and giving each ICD-10 and ATC code a list
of relevant therapy chapters. Each patient case can have the same code listed
several times, one for each line potentially. Each code can also have the same
chapter listed several times. Therefor we created a list of the relevant
therapy chapers for each patient case based on task 1 and 2 results, with a
weight for each with the number of times they are listed. We use the
hierarchical structure of the codes to add relevant chapters for parent codes,
but with a smaller weight (0.1) for each chapter.

We then used the hierarchical structure of therapy chapters to boost parent
chapters if more than one if its sub-chapters were considered relevant. The
parent chapter will get a weight of the max of its current weight and the
weights if its children plus a bonus of the amount of children which are
relevant.

For part B we simply merge the results of task 3 and task 6 A where we
multiply the score of task 3 results by a constant of 200. The constant was
selected to give both set of results fairly equal weight, so that chapters
in both sets gets the highest score.

We will evaluate these results with the methods described in
\autoref{sec:task4}: Precision at ten documents seen and R-precision.


\section{Task 7: Gold standard}
%------------------------------


