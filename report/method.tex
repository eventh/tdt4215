%---------------
\chapter{Method}
%---------------
\label{cha:method}
This section describes methods used to solve the tasks of the assignment, including
preprocessing and parsing of input files.


\section{Preprocessing and parsing}
%----------------------------------
We preprocess and parse input data files before we save them to disk as JSON
files. This is to prevent being able to work on one task at a time, without
having to do parsing each time we want to run a task. We also store task
results as JSON files.

\subsection{ICD-10}
ICD-10 were provided in an ownl2 file. We simply parsed it as a normal XML
file, with the ElementTree module in the Python standard library.
Nodes named umls\_tui, umls\_conceptId, and umls\_atomId we throw away.
Nodes named underterm, synonym, inclusion, and exclusion we store as list of
strings. The rest of the nodes are handled as strings. All these values we
store as attributes on a single ICD object. ICD-10 codes that lack both label
and code\_compacted we also throw away.

\subsection{ATC}
Parsing of ATC is very staight forward. Each line represent a code, title
pair where codes are not unique.

\subsection{Therapy chapters}
Therapy chapters from ``Norsk legemiddelhåndbok'' were provided as\\
HTML files, invalid html5 files in iso-8859-1 charset. We first preprocessed
these files by removing some of the HTML tags to make them easier to parse,
and we converted them to utf-8 charset.

We created a custom parser for parsing therapy chapters, based on\\
Python's HTMLParser. We parse one HTML file at a time, creating Therapy
objects for each chapter or sub*-chapter. The text found in these chapters are
stored on the objects. We stored links as a list on each object, while we
preserved their text in the object text. Sections which list relevant drugs
were removed from the text but stored as they might be useful later.

We manually removed subchapter T17.2 and T19.7 as they had no title nor
contained any text.

\subsection{Patient cases}
Patient cases were provided as a Word file, which included eight cases. We
created a text file for each case, and made sure they were in utf-8 charset.


\section{Stopwords}
%------------------
To reduce the number of terms in documents, and to remove words which provide
little or no relevant information value, we removed stopwords.
We used a list of Norwegian stopwords in both ``bokmål'' and ``nynorsk''
which we found
online\footnote{Stopword source: \url{http://www.wisweb.no/999/147/33899-170.html}},
and we added a few words ourself. A complete list of these stopwords can
be found in \autoref{tab:stopwords} in \autoref{appendix}.


\section{Task 1: Autocoding ICD-10}
%----------------------------------
Describe both methods...

%Describe first methods which are used in several tasks, like Whoosh default
%ranking method BM25F.
%\url{http://packages.python.org/Whoosh/api/scoring.html#whoosh.scoring.BM27F}

\section{Task 2: Autocoding ATC}
%-------------------------------


\section{Task 3: Ranking using vector models}
%--------------------------------------------
\label{sec:task3}
We decided to use a vector model for calculating the similarities between
patient cases and therapy chapters. For each document, both patient cases
and therapy chapters, we created a vector with all the terms and their
TF-IDF value. These document-vectors gives us a pseudo term-document matrix,
without having to create an actual matrix. As there were over 30,000 terms
and almost 1000 documents, a full term-document matrix would have 30M cells.
Our pseudo term-document only have 124,065 term-weight pairs.

We have tested four different varieties of TF-IDF. For TF we tested log
normalization (\( 1 + \log f_{i,j} \)) and raw frequency (\( f_{i,j} \)).
For IDF we tested inverse frequency (\( \log(\frac{N}{n_{i}}) \)) and
probabilistic inverse frequency (\( \log(\frac{N - n_{i}}{n_{i}}) \)).
\begin{description}
	\item[A] log normalization and inverse frequency as seen in \autoref{eq:tfidf}
	\item[B] log normalization and probabilistic inverse frequency
	\item[C] raw frequency and inverse frequency 
	\item[D] raw frequency and probabilistic inverse frequency
\end{description}
Function \( f_{i,j} \) is the frequency of term \( i \) in document
\( j \), \( N \) is the total number of documents, while \( n_{i} \) is the
document frequency of term \( i \).
\begin{equation} \label{eq:tfidf}
	w_{i,j} =
	\begin{cases}
		(1 + \log f_{i,j}) \times \log(\frac{N}{n_{i}}) & \text{if } f_{i,j} > 0 \\
		0												& \text{otherwise}
	\end{cases}
\end{equation}

For each clinical note (which we have defined to be a patient case), we
calculated the similarities with all therapy chapter vectors. The similarity is
calculated as the \emph{cosine of the angle} between the two vectors, as seen
in \autoref{eq:sim}, where \( d_{j} \) is the term vector of therapy chapter \( j \),
\( q \) is a patient case vector and \( t \) is the total number of terms.
\begin{align} \label{eq:sim}
	sim(d_{j}, q) &= \frac{\vec{d_{j}} \bullet \vec{q}}{|\vec{d}| \times |\vec{q}|} \nonumber \\
				  &= \frac{\sum_{i=1}^{t} w_{i,j} \times w_{i,q}}{\sqrt{\sum_{i=1}^{t} w_{i,j}^2} \times \sqrt{\sum_{i=1}^{t} w_{i,q}^2}}
\end{align}
We sorted results based on their similarity score (highest first), and
returned the first 10 results.


\section{Task 4: Evaluation}
%---------------------------
The problem with automatic evaluation is deciding if a retrieved document
is relevant or not. As the queries in this task is patient cases, which is
known beforehand, we can create a criteria for whether a document is relevant
or not. We have simply taken all the words in the patient cases and created a
list of those with medical importance, and if both the query and a retrieved
document contains such a word we deem it relevant. These medical terms are
listed in \autoref{tab:medicalterms} in \hyperref[appendix]{Appendix}.

We have calculated average precision at ten documents seen (P@10) and
R-precision for the four different TF-IDF definitions A to D as described in
\autoref{sec:task3}.

We have also calculated the Kendall tau coefficients for the four ranking
methods. Kendall tau coefficient is a rank correlation metric for automatic
comparing of two ranking methods to determine how differently one varies from
another. It does not consider relevance of retrieved documents, only the
relative ordering of two rankings.


\section{Task 5: Exchange evaluations}
%-------------------------------------


\section{Task 6: Improving the ranking}
%--------------------------------------


\section{Task 7: Gold standard}
%------------------------------


